import numpy as np
import gym
import matplotlib.pyplot as plt
env = gym.make('MountainCar-v0')
/usr/local/lib/python3.10/dist-packages/gym/core.py:317:
DeprecationWarning: WARN: Initializing wrapper in old step API which
returns one bool instead of two. It is recommended to set
`new_step_api=True` to use new step API. This will be the default
behaviour in future.
 deprecation(
/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatib
ility.py:39: DeprecationWarning: WARN: Initializing environment in old
step API which returns one bool instead of two. It is recommended to
set `new_step_api=True` to use new step API. This will be the default
behaviour in future.
 deprecation(
# Parameters
alpha = 0.1 # learning rate
gamma = 0.99 # discount factor
epsilon = 0.1 # exploration rate
num_episodes = 5000
# Function to discretize continuous state space
def discretize_state(state):
 state_adj = (state - env.observation_space.low) /
(env.observation_space.high - env.observation_space.low)
 state_adj = np.round(state_adj * np.array([9, 9]) + np.array([0,
0])).astype(int)
 return state_adj
# Initialize Q-table
Q = np.zeros((10, 10, env.action_space.n))
# Function to select action using epsilon-greedy policy
def select_action(state, epsilon):
 if np.random.uniform(0, 1) < epsilon:
 # Exploration
 return env.action_space.sample()
 else:
 # Exploitation
 return np.argmax(Q[state[0], state[1], :])
# Run Semi-Gradient Sarsa(0) algorithm
rewards = []
steps = []
for i in range(num_episodes):
 state = env.reset()
 state = discretize_state(state)
 done = False
 action = select_action(state, epsilon)
 episode_reward = 0
 episode_steps = 0
 while not done:
 next_state, reward, done, _ = env.step(action)
 next_state = discretize_state(next_state)
 next_action = select_action(next_state, epsilon)
 Q[state[0], state[1], action] += alpha * (reward + gamma *
Q[next_state[0], next_state[1], next_action] - Q[state[0], state[1],
action])
 state = next_state
 action = next_action
 episode_reward += reward
 episode_steps += 1
 rewards.append(episode_reward)
 steps.append(episode_steps)
# Evaluate performance after learning
total_reward = 0
num_episodes = 100
for i in range(num_episodes):
 state = env.reset()
 state = discretize_state(state)
 done = False
 while not done:
 action = np.argmax(Q[state[0], state[1], :])
 next_state, reward, done, _ = env.step(action)
 next_state = discretize_state(next_state)
 state = next_state
 total_reward += reward
print('Average reward:', total_reward / num_episodes)
Average reward: -200.0
# Plot rewards vs. steps
plt.plot(np.cumsum(steps), rewards)
plt.xlabel('Steps')
plt.ylabel('Average Reward (over 100 episodes)')
plt.title('Semi-Gradient Sarsa(0) with Mountain Car v0')
plt.show()
import gym
import numpy as np
env = gym.make('MountainCar-v0')
# Function to convert observation to state
def to_state(observation):
 position, velocity = observation
 state = ((position - env.observation_space.low[0]) /
(env.observation_space.high[0] - env.observation_space.low[0])) * 40
 state = int(state)
 return state
# Function to select an action using epsilon-greedy policy
def epsilon_greedy(Q, state, epsilon):
 if np.random.uniform(0, 1) < epsilon:
 action = env.action_space.sample()
 else:
 action = np.argmax(Q[state])
 return action
# Hyperparameters
alpha = 0.1
gamma = 0.99
lmbda = 0.5
epsilon = 0.1
num_episodes = 1000
# Initialize Q-value function and eligibility trace
Q = np.zeros((40, 40, 3))
E = np.zeros((40, 40, 3))
# Loop over episodes
for i in range(num_episodes):
 observation = env.reset()
 state = to_state(observation)
 action = epsilon_greedy(Q, state, epsilon)
 done = False

def semi_gradient_td_lambda(num_episodes, num_steps):
 value_function = np.zeros(env.observation_space.n)
 eligibility_trace = np.zeros(env.observation_space.n)
 rewards_list = []
 for episode in range(num_episodes):
 state = env.reset()
 episode_rewards = 0
 for step in range(num_steps):
 epsilon = 0.1
 if np.random.uniform(0, 1) < epsilon:
 action = env.action_space.sample()
 else:
 action = np.argmax(value_function[state])
 next_state, reward, done, _ = env.step(action)
 td_error = reward + discount_factor *
value_function[next_state] - value_function[state]
 eligibility_trace[state] = eligibility_trace[state] + 1
 value_function += learning_rate * td_error *
eligibility_trace
 eligibility_trace *= discount_factor * lmbda
 episode_rewards += reward
 state = next_state
 if done:
 break
 if episode % 100 == 0:
 rewards_list.append(np.mean(rewards[-100:]))
def epsilon_greedy_policy(q_values, epsilon):
 if np.random.uniform(0, 1) < epsilon:
 return env.action_space.sample()
 else:
 return np.argmax(q_values)
total_reward = 0
observation = env.reset()
state = to_state(observation)
done = False
while not done:
 action = np.argmax(Q[state])
 observation, reward, done, info = env.step(action)
 total_reward += reward
 state = to_state(observation)
env.close()
print("Total reward:", total_reward)
Total reward: -210
# Plot rewards vs. steps
plt.plot(np.cumsum(steps), rewards)
plt.xlabel('Steps')
plt.ylabel('Average Reward (over 100 episodes)')
plt.title('Semi_gradient_td_lambda (0) with Mountain Car v0')
